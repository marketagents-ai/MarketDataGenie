# Pythonformer Config for HotpotQA Multi-Hop Reasoning
# Multi-hop question answering over multiple documents

# LLM settings
main_model: "Hermes-4-405B"
main_client: "litellm"
main_temperature: 0.7
main_max_tokens: 4096

# Sub-LLM for sub_agent() calls (semantic analysis of documents)
sub_model: "Hermes-4-70B"
sub_client: "litellm"
sub_temperature: 0.3
sub_max_tokens: 2048

# REPL settings
repl:
  server_url: "http://localhost:5003"
  max_output_chars: 8192
  max_turns: 15  # Multi-hop reasoning may need more turns
  timeout_seconds: 180

# Dataset settings
dataset:
  environment: "hotpotqa"
  dataset_name: "hotpotqa/hotpot_qa"
  dataset_config: "distractor"  # Options: "distractor" (10 docs), "fullwiki" (harder)
  dataset_split: "validation"
  field_mapping:
    id: "id"
    prompt: "question"
    expected_answer: "answer"
    context: "context"
  context_processor: "hotpotqa"  # Special handling for HotpotQA context structure
  output_dir: "outputs/pythonformer_hotpotqa"
  output_sharegpt: true
  mask_observations: false
  batch_size: 4
  limit: null
  enable_rewards: false  # Toggle: set to true to enable reward computation
  reward_function: "simple"  # Options: "simple", "efficiency", "normalized"

# Debug settings
debug: false
